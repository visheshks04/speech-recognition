{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpeechRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5LMxuKgVoGw",
        "outputId": "66426c07-5bcc-4f8d-d555-00ef9e3b8aa3"
      },
      "source": [
        "#Update link\n",
        "!wget 'https://raw.githubusercontent.com/visheshks04/speech-recognition/master/data/captions_data.json?token=ALKS45RUMXKDIKIDLV6N2WDBOV4OW'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-24 15:16:17--  https://raw.githubusercontent.com/visheshks04/speech-recognition/master/data/captions_data.json?token=ALKS45RUMXKDIKIDLV6N2WDBOV4OW\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 132177 (129K) [text/plain]\n",
            "Saving to: ‘captions_data.json?token=ALKS45RUMXKDIKIDLV6N2WDBOV4OW’\n",
            "\n",
            "\r          captions_   0%[                    ]       0  --.-KB/s               \rcaptions_data.json? 100%[===================>] 129.08K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-10-24 15:16:17 (5.76 MB/s) - ‘captions_data.json?token=ALKS45RUMXKDIKIDLV6N2WDBOV4OW’ saved [132177/132177]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InWNbnymVqq6"
      },
      "source": [
        "#Update filename here\n",
        "!mv captions_data.json?token=ALKS45RUMXKDIKIDLV6N2WDBOV4OW captions_data.json"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCVZ9EN3f-4G"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWC4Xu8HWFgk",
        "outputId": "6b42b823-53d8-4e6b-b697-1d9b4e2b356c"
      },
      "source": [
        "!pip install pytube"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytube in /usr/local/lib/python3.7/dist-packages (11.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X4o4Uy7gCcC",
        "outputId": "5992bda5-52d0-4af9-e7df-f04633c39500"
      },
      "source": [
        "!pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchaudio) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI4uZhq8V4rV"
      },
      "source": [
        "## Fetch Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "AFKQVMLFV6fF",
        "outputId": "3eedd791-6120-467d-bfe2-ec9fd6872890"
      },
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "url = 'https://youtu.be/HtSuA80QTyo'\n",
        "\n",
        "yt = YouTube(url)\n",
        "print(f'Downloading {yt.title}')\n",
        "audio = yt.streams.get_by_itag(251)\n",
        "audio.download('.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1. Algorithmic Thinking, Peak Finding\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/./1 Algorithmic Thinking Peak Finding.webm'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f28PpnU2XpQZ"
      },
      "source": [
        "# Convert to wav"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9ITS-5hXsQe",
        "outputId": "0a917461-f9fe-4d3c-dd62-0ef0910d6788"
      },
      "source": [
        "import librosa\n",
        "from scipy.io import wavfile\n",
        "\n",
        "aud = librosa.load('1 Algorithmic Thinking Peak Finding.webm')\n",
        "wavfile.write('wavAudio.wav', aud[1], aud[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ez2SFVsW0o9"
      },
      "source": [
        "## Creating JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubDaAiqvWt9x"
      },
      "source": [
        "import json\n",
        "\n",
        "data = []\n",
        "\n",
        "with open('captions_data.json', 'r') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "for event in captions_data['events']:\n",
        "    sample = dict()\n",
        "    sample[\"key\"] = str(event['tStartMs']) + '.wav'\n",
        "    sample[\"text\"] = event['segs'][0]['utf8']\n",
        "    data.append(sample)\n",
        "\n",
        "\n",
        "with open('dataset.json', 'w') as f:\n",
        "    json.dump(data, f, indent=4)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2LPg-Y6XYOX"
      },
      "source": [
        "## Chopping clips"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I54pF48YWda8"
      },
      "source": [
        "!mkdir clips"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBczDSWvYThb"
      },
      "source": [
        "with open('captions_data.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "aud = librosa.load('wavAudio.wav')\n",
        "\n",
        "for event in data['events']:\n",
        "    start_index = event['tStartMs'] * 1e-3 * aud[1]\n",
        "    end_index = (event['tStartMs'] + event['dDurationMs']) * 1e-3 * aud[1]\n",
        "\n",
        "    start_index, end_index = int(start_index), int(end_index)\n",
        "\n",
        "    chopped_sample = aud[0][start_index:end_index]\n",
        "    wavfile.write('clips/{}.wav'.format(event['tStartMs']), aud[1], chopped_sample)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayj0h97QZbdi"
      },
      "source": [
        "## Cleaning Dataset.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU56RXRNZdul"
      },
      "source": [
        "def clean_data(dataset_path):\n",
        "\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    for event in data:\n",
        "        sentence = event['text']\n",
        "        sentence = sentence.replace('\\n', ' ')\n",
        "        sentence = remove_caps(sentence)\n",
        "        sentence = remove_special_chars(sentence)\n",
        "        sentence = sentence.lower()\n",
        "        event['text'] = sentence\n",
        "\n",
        "    with open(dataset_path, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "\n",
        "def remove_caps(string):\n",
        "    string = string.split()\n",
        "    for word in string:\n",
        "        if(word.isupper()):\n",
        "            string.remove(word)\n",
        "    string = \" \".join(string)\n",
        "    return string\n",
        "\n",
        "def remove_special_chars(string):\n",
        "\n",
        "    removables = ['.', ',', ';', '\"', '\\'', ':', '<', '>', '?', '/', '\\\\', '[', ']', '{', '}', '-', '_', '+', '=', '(', ')', '!', '@', '#', '$', '%', '^', '&', '*', '~', '`']\n",
        "\n",
        "    string = list(string)\n",
        "\n",
        "    for ch in string:\n",
        "        if ch in removables:\n",
        "            string.remove(ch)\n",
        "\n",
        "    string = \"\".join(string)\n",
        "    return string"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaTilLvFZ89B"
      },
      "source": [
        "clean_data('dataset.json')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXO4ERqjf0q3"
      },
      "source": [
        "##################################################################################################################"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSyXQlNqhFtJ"
      },
      "source": [
        "import torch\n",
        "\n",
        "class TextProcess:\n",
        "\tdef __init__(self):\n",
        "\t\tchar_map_str = \"\"\"\n",
        "\t\t' 0\n",
        "\t\t<SPACE> 1\n",
        "\t\ta 2\n",
        "\t\tb 3\n",
        "\t\tc 4\n",
        "\t\td 5\n",
        "\t\te 6\n",
        "\t\tf 7\n",
        "\t\tg 8\n",
        "\t\th 9\n",
        "\t\ti 10\n",
        "\t\tj 11\n",
        "\t\tk 12\n",
        "\t\tl 13\n",
        "\t\tm 14\n",
        "\t\tn 15\n",
        "\t\to 16\n",
        "\t\tp 17\n",
        "\t\tq 18\n",
        "\t\tr 19\n",
        "\t\ts 20\n",
        "\t\tt 21\n",
        "\t\tu 22\n",
        "\t\tv 23\n",
        "\t\tw 24\n",
        "\t\tx 25\n",
        "\t\ty 26\n",
        "\t\tz 27\n",
        "\t\t\"\"\"\n",
        "\t\tself.char_map = {}\n",
        "\t\tself.index_map = {}\n",
        "\t\tfor line in char_map_str.strip().split('\\n'):\n",
        "\t\t\tch, index = line.split()\n",
        "\t\t\tself.char_map[ch] = int(index)\n",
        "\t\t\tself.index_map[int(index)] = ch\n",
        "\t\tself.index_map[1] = ' '\n",
        "\n",
        "\tdef text_to_int_sequence(self, text):\n",
        "\t\t\"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "\t\tint_sequence = []\n",
        "\t\tfor c in text:\n",
        "\t\t\tif c == ' ':\n",
        "\t\t\t\tch = self.char_map['<SPACE>']\n",
        "\t\t\telse:\n",
        "\t\t\t\tch = self.char_map[c]\n",
        "\t\t\tint_sequence.append(ch)\n",
        "\t\treturn int_sequence\n",
        "\n",
        "\tdef int_to_text_sequence(self, labels):\n",
        "\t\t\"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "\t\tstring = []\n",
        "\t\tfor i in labels:\n",
        "\t\t\tstring.append(self.index_map[i])\n",
        "\t\treturn ''.join(string).replace('<SPACE>', ' ')\n",
        "\n",
        "\n",
        "textprocess = TextProcess()\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(textprocess.int_to_text_sequence(\n",
        "\t\t\t\tlabels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(textprocess.int_to_text_sequence(decode))\n",
        "\treturn decodes, targets"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BsWQ5EIhIuj"
      },
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# NOTE: add time stretch\n",
        "class SpecAugment(nn.Module):\n",
        "\n",
        "    def __init__(self, rate, policy=3, freq_mask=15, time_mask=35):\n",
        "        super(SpecAugment, self).__init__()\n",
        "\n",
        "        self.rate = rate\n",
        "\n",
        "        self.specaug = nn.Sequential(\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=time_mask)\n",
        "        )\n",
        "\n",
        "        self.specaug2 = nn.Sequential(\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=time_mask),\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=time_mask)\n",
        "        )\n",
        "\n",
        "        policies = { 1: self.policy1, 2: self.policy2, 3: self.policy3 }\n",
        "        self._forward = policies[policy]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward(x)\n",
        "\n",
        "    def policy1(self, x):\n",
        "        probability = torch.rand(1, 1).item()\n",
        "        if self.rate > probability:\n",
        "            return  self.specaug(x)\n",
        "        return x\n",
        "\n",
        "    def policy2(self, x):\n",
        "        probability = torch.rand(1, 1).item()\n",
        "        if self.rate > probability:\n",
        "            return  self.specaug2(x)\n",
        "        return x\n",
        "\n",
        "    def policy3(self, x):\n",
        "        probability = torch.rand(1, 1).item()\n",
        "        if probability > 0.5:\n",
        "            return self.policy1(x)\n",
        "        return self.policy2(x)\n",
        "\n",
        "\n",
        "class LogMelSpec(nn.Module):\n",
        "\n",
        "    def __init__(self, sample_rate=8000, n_mels=128, win_length=160, hop_length=80):\n",
        "        super(LogMelSpec, self).__init__()\n",
        "        self.transform = torchaudio.transforms.MelSpectrogram(\n",
        "                            sample_rate=sample_rate, n_mels=n_mels,\n",
        "                            win_length=win_length, hop_length=hop_length)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transform(x)  # mel spectrogram\n",
        "        x = np.log(x + 1e-14)  # logrithmic, add small value to avoid inf\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_featurizer(sample_rate, n_feats=81):\n",
        "    return LogMelSpec(sample_rate=sample_rate, n_mels=n_feats,  win_length=160, hop_length=80)\n",
        "\n",
        "\n",
        "class Data(torch.utils.data.Dataset):\n",
        "\n",
        "    # this makes it easier to be ovveride in argparse\n",
        "    parameters = {\n",
        "        \"sample_rate\": 8000, \"n_feats\": 81,\n",
        "        \"specaug_rate\": 0.5, \"specaug_policy\": 3,\n",
        "        \"time_mask\": 70, \"freq_mask\": 15 \n",
        "    }\n",
        "\n",
        "    def __init__(self, json_path, sample_rate, n_feats, specaug_rate, specaug_policy,\n",
        "                time_mask, freq_mask, valid=False, shuffle=True, text_to_int=True, log_ex=True):\n",
        "        self.log_ex = log_ex\n",
        "        self.text_process = TextProcess()\n",
        "\n",
        "        print(\"Loading data json file from\", json_path)\n",
        "        self.data = pd.read_json(json_path, lines=True)\n",
        "\n",
        "        if valid:\n",
        "            self.audio_transforms = torch.nn.Sequential(\n",
        "                LogMelSpec(sample_rate=sample_rate, n_mels=n_feats,  win_length=160, hop_length=80)\n",
        "            )\n",
        "        else:\n",
        "            self.audio_transforms = torch.nn.Sequential(\n",
        "                LogMelSpec(sample_rate=sample_rate, n_mels=n_feats,  win_length=160, hop_length=80),\n",
        "                SpecAugment(specaug_rate, specaug_policy, freq_mask, time_mask)\n",
        "            )\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.item()\n",
        "\n",
        "        try:\n",
        "            file_path = self.data.key.iloc[idx]\n",
        "            waveform, _ = torchaudio.load(file_path)\n",
        "            label = self.text_process.text_to_int_sequence(self.data['text'].iloc[idx])\n",
        "            spectrogram = self.audio_transforms(waveform) # (channel, feature, time)\n",
        "            spec_len = spectrogram.shape[-1] // 2\n",
        "            label_len = len(label)\n",
        "            if spec_len < label_len:\n",
        "                raise Exception('spectrogram len is bigger then label len')\n",
        "            if spectrogram.shape[0] > 1:\n",
        "                raise Exception('dual channel, skipping audio file %s'%file_path)\n",
        "            if spectrogram.shape[2] > 1650:\n",
        "                raise Exception('spectrogram to big. size %s'%spectrogram.shape[2])\n",
        "            if label_len == 0:\n",
        "                raise Exception('label len is zero... skipping %s'%file_path)\n",
        "        except Exception as e:\n",
        "            if self.log_ex:\n",
        "                print(str(e), file_path)\n",
        "            return self.__getitem__(idx - 1 if idx != 0 else idx + 1)  \n",
        "        return spectrogram, label, spec_len, label_len\n",
        "\n",
        "    def describe(self):\n",
        "        return self.data.describe()\n",
        "\n",
        "\n",
        "def collate_fn_padd(data):\n",
        "    '''\n",
        "    Padds batch of variable length\n",
        "\n",
        "    note: it converts things ToTensor manually here since the ToTensor transform\n",
        "    assume it takes in images rather than arbitrary tensors.\n",
        "    '''\n",
        "    # print(data)\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (spectrogram, label, input_length, label_length) in data:\n",
        "        if spectrogram is None:\n",
        "            continue\n",
        "       # print(spectrogram.shape)\n",
        "        spectrograms.append(spectrogram.squeeze(0).transpose(0, 1))\n",
        "        labels.append(torch.Tensor(label))\n",
        "        input_lengths.append(input_length)\n",
        "        label_lengths.append(label_length)\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "    input_lengths = input_lengths\n",
        "    # print(spectrograms.shape)\n",
        "    label_lengths = label_lengths\n",
        "    # ## compute mask\n",
        "    # mask = (batch != 0).cuda(gpu)\n",
        "    # return batch, lengths, mask\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQbXsCRh6MYX"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3JrMyc15u1C"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class ActDropNormCNN1D(nn.Module):\n",
        "    def __init__(self, n_feats, dropout, keep_shape=False):\n",
        "        super(ActDropNormCNN1D, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(n_feats)\n",
        "        self.keep_shape = keep_shape\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        # x = self.norm(self.dropout(F.gelu(x)))\n",
        "        x = self.dropout(F.gelu(self.norm(x)))\n",
        "        if self.keep_shape:\n",
        "            return x.transpose(1, 2)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "class SpeechRecognition(nn.Module):\n",
        "    hyper_parameters = {\n",
        "        \"num_classes\": 29,\n",
        "        \"n_feats\": 81,\n",
        "        \"dropout\": 0.1,\n",
        "        \"hidden_size\": 1024,\n",
        "        \"num_layers\": 1\n",
        "    }\n",
        "\n",
        "    def __init__(self, hidden_size, num_classes, n_feats, num_layers, dropout):\n",
        "        super(SpeechRecognition, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(n_feats, n_feats, 10, 2, padding=10//2),\n",
        "            ActDropNormCNN1D(n_feats, dropout),\n",
        "        )\n",
        "        self.dense = nn.Sequential(\n",
        "            nn.Linear(n_feats, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, dropout=0.0,\n",
        "                            bidirectional=False)\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.final_fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        n, hs = self.num_layers, self.hidden_size\n",
        "        return (torch.zeros(n*1, batch_size, hs),\n",
        "                torch.zeros(n*1, batch_size, hs))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.squeeze(1)  # batch, feature, time\n",
        "        x = self.cnn(x) # batch, time, feature\n",
        "        x = self.dense(x) # batch, time, feature\n",
        "        x = x.transpose(0, 1) # time, batch, feature\n",
        "        out, (hn, cn) = self.lstm(x, hidden)\n",
        "        x = self.dropout2(F.gelu(self.layer_norm2(out)))  # (time, batch, n_class)\n",
        "        return self.final_fc(x), (hn, cn)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-3-hwpp58i7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}