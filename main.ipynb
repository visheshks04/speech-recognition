{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpeechRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5LMxuKgVoGw",
        "outputId": "d34ea2da-2bb3-45c3-e634-4a4a799bc9d6"
      },
      "source": [
        "#Update link\n",
        "!wget 'https://raw.githubusercontent.com/visheshks04/speech-recognition/master/data/captions_data.json?token=ALKS45TCFDUJ5Z6YUTAFPVDBO3UUU'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-25 17:28:04--  https://raw.githubusercontent.com/visheshks04/speech-recognition/master/data/captions_data.json?token=ALKS45TCFDUJ5Z6YUTAFPVDBO3UUU\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 132177 (129K) [text/plain]\n",
            "Saving to: ‘captions_data.json?token=ALKS45TCFDUJ5Z6YUTAFPVDBO3UUU’\n",
            "\n",
            "captions_data.json? 100%[===================>] 129.08K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-10-25 17:28:04 (5.60 MB/s) - ‘captions_data.json?token=ALKS45TCFDUJ5Z6YUTAFPVDBO3UUU’ saved [132177/132177]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InWNbnymVqq6"
      },
      "source": [
        "#Update filename here\n",
        "!mv captions_data.json?token=ALKS45TCFDUJ5Z6YUTAFPVDBO3UUU captions_data.json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCVZ9EN3f-4G"
      },
      "source": [
        "## Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWC4Xu8HWFgk",
        "outputId": "69525ce1-c39a-4ed4-a7a8-d0639f387a89"
      },
      "source": [
        "!pip3 install pytube"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytube in /usr/local/lib/python3.7/dist-packages (11.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-X4o4Uy7gCcC",
        "outputId": "64165807-c3dc-46c0-ec22-af243df2d99d"
      },
      "source": [
        "!pip3 install torchaudio"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchaudio) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYH2o6Eh7hOg",
        "outputId": "c3eb7265-2452-4be2-e47a-857545376a4f"
      },
      "source": [
        "!pip3 install git+https://github.com/PyTorchLightning/pytorch-lightning fsspec --no-deps --target=$nb_path"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PyTorchLightning/pytorch-lightning\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning to /tmp/pip-req-build-f_we_o_k\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-req-build-f_we_o_k\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (2021.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI4uZhq8V4rV"
      },
      "source": [
        "## Fetch Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "AFKQVMLFV6fF",
        "outputId": "ba638d3c-b9e0-426f-e3ad-3338818a3c7d"
      },
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "url = 'https://youtu.be/HtSuA80QTyo'\n",
        "\n",
        "yt = YouTube(url)\n",
        "print(f'Downloading {yt.title}')\n",
        "audio = yt.streams.get_by_itag(251)\n",
        "audio.download('.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1. Algorithmic Thinking, Peak Finding\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/./1 Algorithmic Thinking Peak Finding.webm'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f28PpnU2XpQZ"
      },
      "source": [
        "# Convert to wav"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9ITS-5hXsQe",
        "outputId": "1752b855-7af8-499c-cd49-e2cd71691d35"
      },
      "source": [
        "import librosa\n",
        "from scipy.io import wavfile\n",
        "\n",
        "aud = librosa.load('1 Algorithmic Thinking Peak Finding.webm')\n",
        "wavfile.write('wavAudio.wav', aud[1], aud[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ez2SFVsW0o9"
      },
      "source": [
        "## Creating JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubDaAiqvWt9x"
      },
      "source": [
        "import json\n",
        "\n",
        "data = []\n",
        "\n",
        "with open('captions_data.json', 'r') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "for event in captions_data['events']:\n",
        "    sample = dict()\n",
        "    sample[\"key\"] = str(event['tStartMs']) + '.wav'\n",
        "    sample[\"text\"] = event['segs'][0]['utf8']\n",
        "    data.append(sample)\n",
        "\n",
        "\n",
        "with open('dataset.json', 'w') as f:\n",
        "    json.dump(data, f, indent=4)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2LPg-Y6XYOX"
      },
      "source": [
        "## Chopping clips"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I54pF48YWda8"
      },
      "source": [
        "!mkdir clips"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBczDSWvYThb"
      },
      "source": [
        "with open('captions_data.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "aud = librosa.load('wavAudio.wav')\n",
        "\n",
        "for event in data['events']:\n",
        "    start_index = event['tStartMs'] * 1e-3 * aud[1]\n",
        "    end_index = (event['tStartMs'] + event['dDurationMs']) * 1e-3 * aud[1]\n",
        "\n",
        "    start_index, end_index = int(start_index), int(end_index)\n",
        "\n",
        "    chopped_sample = aud[0][start_index:end_index]\n",
        "    wavfile.write('clips/{}.wav'.format(event['tStartMs']), aud[1], chopped_sample)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayj0h97QZbdi"
      },
      "source": [
        "## Cleaning Dataset.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU56RXRNZdul"
      },
      "source": [
        "def clean_data(dataset_path):\n",
        "\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    for event in data:\n",
        "        sentence = event['text']\n",
        "        sentence = sentence.replace('\\n', ' ')\n",
        "        sentence = remove_caps(sentence)\n",
        "        sentence = remove_special_chars(sentence)\n",
        "        sentence = sentence.lower()\n",
        "        event['text'] = sentence\n",
        "\n",
        "    with open(dataset_path, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "\n",
        "def remove_caps(string):\n",
        "    string = string.split()\n",
        "    for word in string:\n",
        "        if(word.isupper()):\n",
        "            string.remove(word)\n",
        "    string = \" \".join(string)\n",
        "    return string\n",
        "\n",
        "def remove_special_chars(string):\n",
        "\n",
        "    removables = ['.', ',', ';', '\"', '\\'', ':', '<', '>', '?', '/', '\\\\', '[', ']', '{', '}', '-', '_', '+', '=', '(', ')', '!', '@', '#', '$', '%', '^', '&', '*', '~', '`']\n",
        "\n",
        "    string = list(string)\n",
        "\n",
        "    for ch in string:\n",
        "        if ch in removables:\n",
        "            string.remove(ch)\n",
        "\n",
        "    string = \"\".join(string)\n",
        "    return string"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaTilLvFZ89B"
      },
      "source": [
        "clean_data('dataset.json')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SXiK0ZJnNOo"
      },
      "source": [
        "## Train-Dev-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXO4ERqjf0q3"
      },
      "source": [
        "def train_dev_test(file_path, train_size = 0.8, dev_size = 0.1):\n",
        "  with open(file_path, 'r') as f:\n",
        "    ds = json.load(f)\n",
        "\n",
        "  n = len(ds)\n",
        "\n",
        "  train_ind = int(n*train_size)\n",
        "  dev_ind = int(train_ind + n*dev_size)\n",
        "\n",
        "  train_set = ds[:train_ind]\n",
        "  dev_set = ds[train_ind:dev_ind]\n",
        "  test_set = ds[dev_ind:]\n",
        "\n",
        "  with open('train.json', 'w') as f:\n",
        "    json.dump(train_set, f, indent = 4)\n",
        "\n",
        "  with open('dev.json', 'w') as f:\n",
        "    json.dump(dev_set, f, indent = 4)\n",
        "\n",
        "  with open('test.json', 'w') as f:\n",
        "    json.dump(test_set, f, indent = 4)\n",
        "\n",
        "\n",
        "train_dev_test('dataset.json')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSyXQlNqhFtJ"
      },
      "source": [
        "import torch\n",
        "\n",
        "class TextProcess:\n",
        "\tdef __init__(self):\n",
        "\t\tchar_map_str = \"\"\"\n",
        "\t\t' 0\n",
        "\t\t<SPACE> 1\n",
        "\t\ta 2\n",
        "\t\tb 3\n",
        "\t\tc 4\n",
        "\t\td 5\n",
        "\t\te 6\n",
        "\t\tf 7\n",
        "\t\tg 8\n",
        "\t\th 9\n",
        "\t\ti 10\n",
        "\t\tj 11\n",
        "\t\tk 12\n",
        "\t\tl 13\n",
        "\t\tm 14\n",
        "\t\tn 15\n",
        "\t\to 16\n",
        "\t\tp 17\n",
        "\t\tq 18\n",
        "\t\tr 19\n",
        "\t\ts 20\n",
        "\t\tt 21\n",
        "\t\tu 22\n",
        "\t\tv 23\n",
        "\t\tw 24\n",
        "\t\tx 25\n",
        "\t\ty 26\n",
        "\t\tz 27\n",
        "\t\t\"\"\"\n",
        "\t\tself.char_map = {}\n",
        "\t\tself.index_map = {}\n",
        "\t\tfor line in char_map_str.strip().split('\\n'):\n",
        "\t\t\tch, index = line.split()\n",
        "\t\t\tself.char_map[ch] = int(index)\n",
        "\t\t\tself.index_map[int(index)] = ch\n",
        "\t\tself.index_map[1] = ' '\n",
        "\n",
        "\tdef text_to_int_sequence(self, text):\n",
        "\t\t\"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "\t\tint_sequence = []\n",
        "\t\tfor c in text:\n",
        "\t\t\tif c == ' ':\n",
        "\t\t\t\tch = self.char_map['<SPACE>']\n",
        "\t\t\telse:\n",
        "\t\t\t\tch = self.char_map[c]\n",
        "\t\t\tint_sequence.append(ch)\n",
        "\t\treturn int_sequence\n",
        "\n",
        "\tdef int_to_text_sequence(self, labels):\n",
        "\t\t\"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "\t\tstring = []\n",
        "\t\tfor i in labels:\n",
        "\t\t\tstring.append(self.index_map[i])\n",
        "\t\treturn ''.join(string).replace('<SPACE>', ' ')\n",
        "\n",
        "\n",
        "textprocess = TextProcess()\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(textprocess.int_to_text_sequence(\n",
        "\t\t\t\tlabels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(textprocess.int_to_text_sequence(decode))\n",
        "\treturn decodes, targets"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BsWQ5EIhIuj"
      },
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# NOTE: add time stretch\n",
        "class SpecAugment(nn.Module):\n",
        "\n",
        "    def __init__(self, rate, policy=3, freq_mask=15, time_mask=35):\n",
        "        super(SpecAugment, self).__init__()\n",
        "\n",
        "        self.rate = rate\n",
        "\n",
        "        self.specaug = nn.Sequential(\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=time_mask)\n",
        "        )\n",
        "\n",
        "        self.specaug2 = nn.Sequential(\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=time_mask),\n",
        "            torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask),\n",
        "            torchaudio.transforms.TimeMasking(time_mask_param=time_mask)\n",
        "        )\n",
        "\n",
        "        policies = { 1: self.policy1, 2: self.policy2, 3: self.policy3 }\n",
        "        self._forward = policies[policy]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward(x)\n",
        "\n",
        "    def policy1(self, x):\n",
        "        probability = torch.rand(1, 1).item()\n",
        "        if self.rate > probability:\n",
        "            return  self.specaug(x)\n",
        "        return x\n",
        "\n",
        "    def policy2(self, x):\n",
        "        probability = torch.rand(1, 1).item()\n",
        "        if self.rate > probability:\n",
        "            return  self.specaug2(x)\n",
        "        return x\n",
        "\n",
        "    def policy3(self, x):\n",
        "        probability = torch.rand(1, 1).item()\n",
        "        if probability > 0.5:\n",
        "            return self.policy1(x)\n",
        "        return self.policy2(x)\n",
        "\n",
        "\n",
        "class LogMelSpec(nn.Module):\n",
        "\n",
        "    def __init__(self, sample_rate=8000, n_mels=128, win_length=160, hop_length=80):\n",
        "        super(LogMelSpec, self).__init__()\n",
        "        self.transform = torchaudio.transforms.MelSpectrogram(\n",
        "                            sample_rate=sample_rate, n_mels=n_mels,\n",
        "                            win_length=win_length, hop_length=hop_length)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.transform(x)  # mel spectrogram\n",
        "        x = np.log(x + 1e-14)  # logrithmic, add small value to avoid inf\n",
        "        return x\n",
        "\n",
        "\n",
        "def get_featurizer(sample_rate, n_feats=81):\n",
        "    return LogMelSpec(sample_rate=sample_rate, n_mels=n_feats,  win_length=160, hop_length=80)\n",
        "\n",
        "\n",
        "class Data(torch.utils.data.Dataset):\n",
        "\n",
        "    # this makes it easier to be ovveride in argparse\n",
        "    parameters = {\n",
        "        \"sample_rate\": 8000, \"n_feats\": 81,\n",
        "        \"specaug_rate\": 0.5, \"specaug_policy\": 3,\n",
        "        \"time_mask\": 70, \"freq_mask\": 15 \n",
        "    }\n",
        "\n",
        "    def __init__(self, json_path, sample_rate, n_feats, specaug_rate, specaug_policy,\n",
        "                time_mask, freq_mask, valid=False, shuffle=True, text_to_int=True, log_ex=True):\n",
        "        self.log_ex = log_ex\n",
        "        self.text_process = TextProcess()\n",
        "\n",
        "        print(\"Loading data json file from\", json_path)\n",
        "        self.data = pd.read_json(json_path, lines=True)\n",
        "\n",
        "        if valid:\n",
        "            self.audio_transforms = torch.nn.Sequential(\n",
        "                LogMelSpec(sample_rate=sample_rate, n_mels=n_feats,  win_length=160, hop_length=80)\n",
        "            )\n",
        "        else:\n",
        "            self.audio_transforms = torch.nn.Sequential(\n",
        "                LogMelSpec(sample_rate=sample_rate, n_mels=n_feats,  win_length=160, hop_length=80),\n",
        "                SpecAugment(specaug_rate, specaug_policy, freq_mask, time_mask)\n",
        "            )\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.item()\n",
        "\n",
        "        try:\n",
        "            file_path = self.data.key.iloc[idx]\n",
        "            waveform, _ = torchaudio.load(file_path)\n",
        "            label = self.text_process.text_to_int_sequence(self.data['text'].iloc[idx])\n",
        "            spectrogram = self.audio_transforms(waveform) # (channel, feature, time)\n",
        "            spec_len = spectrogram.shape[-1] // 2\n",
        "            label_len = len(label)\n",
        "            if spec_len < label_len:\n",
        "                raise Exception('spectrogram len is bigger then label len')\n",
        "            if spectrogram.shape[0] > 1:\n",
        "                raise Exception('dual channel, skipping audio file %s'%file_path)\n",
        "            if spectrogram.shape[2] > 1650:\n",
        "                raise Exception('spectrogram to big. size %s'%spectrogram.shape[2])\n",
        "            if label_len == 0:\n",
        "                raise Exception('label len is zero... skipping %s'%file_path)\n",
        "        except Exception as e:\n",
        "            if self.log_ex:\n",
        "                print(str(e), file_path)\n",
        "            return self.__getitem__(idx - 1 if idx != 0 else idx + 1)  \n",
        "        return spectrogram, label, spec_len, label_len\n",
        "\n",
        "    def describe(self):\n",
        "        return self.data.describe()\n",
        "\n",
        "\n",
        "def collate_fn_padd(data):\n",
        "    '''\n",
        "    Padds batch of variable length\n",
        "\n",
        "    note: it converts things ToTensor manually here since the ToTensor transform\n",
        "    assume it takes in images rather than arbitrary tensors.\n",
        "    '''\n",
        "    # print(data)\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (spectrogram, label, input_length, label_length) in data:\n",
        "        if spectrogram is None:\n",
        "            continue\n",
        "       # print(spectrogram.shape)\n",
        "        spectrograms.append(spectrogram.squeeze(0).transpose(0, 1))\n",
        "        labels.append(torch.Tensor(label))\n",
        "        input_lengths.append(input_length)\n",
        "        label_lengths.append(label_length)\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "    input_lengths = input_lengths\n",
        "    # print(spectrograms.shape)\n",
        "    label_lengths = label_lengths\n",
        "    # ## compute mask\n",
        "    # mask = (batch != 0).cuda(gpu)\n",
        "    # return batch, lengths, mask\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQbXsCRh6MYX"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3JrMyc15u1C"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class ActDropNormCNN1D(nn.Module):\n",
        "    def __init__(self, n_feats, dropout, keep_shape=False):\n",
        "        super(ActDropNormCNN1D, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = nn.LayerNorm(n_feats)\n",
        "        self.keep_shape = keep_shape\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        # x = self.norm(self.dropout(F.gelu(x)))\n",
        "        x = self.dropout(F.gelu(self.norm(x)))\n",
        "        if self.keep_shape:\n",
        "            return x.transpose(1, 2)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "\n",
        "class SpeechRecognition(nn.Module):\n",
        "    hyper_parameters = {\n",
        "        \"num_classes\": 29,\n",
        "        \"n_feats\": 81,\n",
        "        \"dropout\": 0.1,\n",
        "        \"hidden_size\": 1024,\n",
        "        \"num_layers\": 1\n",
        "    }\n",
        "\n",
        "    def __init__(self, hidden_size, num_classes, n_feats, num_layers, dropout):\n",
        "        super(SpeechRecognition, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(n_feats, n_feats, 10, 2, padding=10//2),\n",
        "            ActDropNormCNN1D(n_feats, dropout),\n",
        "        )\n",
        "        self.dense = nn.Sequential(\n",
        "            nn.Linear(n_feats, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, dropout=0.0,\n",
        "                            bidirectional=False)\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.final_fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def _init_hidden(self, batch_size):\n",
        "        n, hs = self.num_layers, self.hidden_size\n",
        "        return (torch.zeros(n*1, batch_size, hs),\n",
        "                torch.zeros(n*1, batch_size, hs))\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = x.squeeze(1)  # batch, feature, time\n",
        "        x = self.cnn(x) # batch, time, feature\n",
        "        x = self.dense(x) # batch, time, feature\n",
        "        x = x.transpose(0, 1) # time, batch, feature\n",
        "        out, (hn, cn) = self.lstm(x, hidden)\n",
        "        x = self.dropout2(F.gelu(self.layer_norm2(out)))  # (time, batch, n_class)\n",
        "        return self.final_fc(x), (hn, cn)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "n-3-hwpp58i7",
        "outputId": "5769cf93-42a7-4d70-d215-a62371619106"
      },
      "source": [
        "import os\n",
        "import ast\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from pytorch_lightning import Trainer\n",
        "from argparse import ArgumentParser\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "class SpeechModule(LightningModule):\n",
        "\n",
        "    def __init__(self, model, args):\n",
        "        super(SpeechModule, self).__init__()\n",
        "        self.model = model\n",
        "        self.criterion = nn.CTCLoss(blank=28, zero_infinity=True)\n",
        "        self.args = args\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        return self.model(x, hidden)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        self.optimizer = optim.AdamW(self.model.parameters(), self.args.learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                                        self.optimizer, mode='min',\n",
        "                                        factor=0.50, patience=6)\n",
        "        return [self.optimizer], [self.scheduler]\n",
        "\n",
        "    def step(self, batch):\n",
        "        spectrograms, labels, input_lengths, label_lengths = batch \n",
        "        bs = spectrograms.shape[0]\n",
        "        hidden = self.model._init_hidden(bs)\n",
        "        hn, c0 = hidden[0].to(self.device), hidden[1].to(self.device)\n",
        "        output, _ = self(spectrograms, (hn, c0))\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        loss = self.criterion(output, labels, input_lengths, label_lengths)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.step(batch)\n",
        "        logs = {'loss': loss, 'lr': self.optimizer.param_groups[0]['lr'] }\n",
        "        return {'loss': loss, 'log': logs}\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        d_params = Data.parameters\n",
        "        d_params.update(self.args.dparams_override)\n",
        "        train_dataset = Data(json_path=self.args.train_file, **d_params)\n",
        "        return DataLoader(dataset=train_dataset,\n",
        "                            batch_size=self.args.batch_size,\n",
        "                            num_workers=self.args.data_workers,\n",
        "                            pin_memory=True,\n",
        "                            collate_fn=collate_fn_padd)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.step(batch)\n",
        "        return {'val_loss': loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        self.scheduler.step(avg_loss)\n",
        "        tensorboard_logs = {'val_loss': avg_loss}\n",
        "        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        d_params = Data.parameters\n",
        "        d_params.update(self.args.dparams_override)\n",
        "        test_dataset = Data(json_path=self.args.valid_file, **d_params, valid=True)\n",
        "        return DataLoader(dataset=test_dataset,\n",
        "                            batch_size=self.args.batch_size,\n",
        "                            num_workers=self.args.data_workers,\n",
        "                            collate_fn=collate_fn_padd,\n",
        "                            pin_memory=True)\n",
        "\n",
        "\n",
        "def checkpoint_callback(args):\n",
        "    return ModelCheckpoint(\n",
        "        filepath=args.save_model_path,\n",
        "        save_top_k=True,\n",
        "        verbose=True,\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        prefix=''\n",
        "    )\n",
        "\n",
        "def main(args):\n",
        "    h_params = SpeechRecognition.hyper_parameters\n",
        "    h_params.update(args.hparams_override)\n",
        "    model = SpeechRecognition(**h_params)\n",
        "\n",
        "    if args.load_model_from:\n",
        "        speech_module = SpeechModule.load_from_checkpoint(args.load_model_from, model=model, args=args)\n",
        "    else:\n",
        "        speech_module = SpeechModule(model, args)\n",
        "\n",
        "    logger = TensorBoardLogger(args.logdir, name='speech_recognition')\n",
        "    trainer = Trainer(logger=logger)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=args.epochs, gpus=args.gpus,\n",
        "        num_nodes=args.nodes, distributed_backend=None,\n",
        "        logger=logger, gradient_clip_val=1.0,\n",
        "        val_check_interval=args.valid_every,\n",
        "        checkpoint_callback=checkpoint_callback(args),\n",
        "        resume_from_checkpoint=args.resume_from_checkpoint\n",
        "    )\n",
        "    trainer.fit(speech_module)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     parser = ArgumentParser()\n",
        "#     # distributed training setup\n",
        "#     parser.add_argument('-n', '--nodes', default=1, type=int, help='number of data loading workers')\n",
        "#     parser.add_argument('-g', '--gpus', default=1, type=int, help='number of gpus per node')\n",
        "#     parser.add_argument('-w', '--data_workers', default=0, type=int,\n",
        "#                         help='n data loading workers, default 0 = main process only')\n",
        "#     parser.add_argument('-db', '--dist_backend', default='ddp', type=str,\n",
        "#                         help='which distributed backend to use. defaul ddp')\n",
        "\n",
        "#     # train and valid\n",
        "#     parser.add_argument('--train_file', default=None, required=True, type=str,\n",
        "#                         help='json file to load training data')\n",
        "#     parser.add_argument('--valid_file', default=None, required=True, type=str,\n",
        "#                         help='json file to load testing data')\n",
        "#     parser.add_argument('--valid_every', default=1000, required=False, type=int,\n",
        "#                         help='valid after every N iteration')\n",
        "\n",
        "#     # dir and path for models and logs\n",
        "#     parser.add_argument('--save_model_path', default=None, required=True, type=str,\n",
        "#                         help='path to save model')\n",
        "#     parser.add_argument('--load_model_from', default=None, required=False, type=str,\n",
        "#                         help='path to load a pretrain model to continue training')\n",
        "#     parser.add_argument('--resume_from_checkpoint', default=None, required=False, type=str,\n",
        "#                         help='check path to resume from')\n",
        "#     parser.add_argument('--logdir', default='tb_logs', required=False, type=str,\n",
        "#                         help='path to save logs')\n",
        "    \n",
        "#     # general\n",
        "#     parser.add_argument('--epochs', default=10, type=int, help='number of total epochs to run')\n",
        "#     parser.add_argument('--batch_size', default=64, type=int, help='size of batch')\n",
        "#     parser.add_argument('--learning_rate', default=1e-3, type=float, help='learning rate')\n",
        "#     parser.add_argument('--pct_start', default=0.3, type=float, help='percentage of growth phase in one cycle')\n",
        "#     parser.add_argument('--div_factor', default=100, type=int, help='div factor for one cycle')\n",
        "#     parser.add_argument(\"--hparams_override\", default=\"{}\", type=str, required=False,\n",
        "# \t\thelp='override the hyper parameters, should be in form of dict. ie. {\"attention_layers\": 16 }')\n",
        "#     parser.add_argument(\"--dparams_override\", default=\"{}\", type=str, required=False,\n",
        "# \t\thelp='override the data parameters, should be in form of dict. ie. {\"sample_rate\": 8000 }')\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "#     args.hparams_override = ast.literal_eval(args.hparams_override)\n",
        "#     args.dparams_override = ast.literal_eval(args.dparams_override)\n",
        "\n",
        "\n",
        "#     if args.save_model_path:\n",
        "#        if not os.path.isdir(os.path.dirname(args.save_model_path)):\n",
        "#            raise Exception(\"the directory for path {} does not exist\".format(args.save_model_path))\n",
        "\n",
        "#     main(args)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-7de2a1f98170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0margparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArgumentParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0m_PROJECT_ROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_PACKAGE_ROOT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLightningDataModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLightningModule\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_stats_monitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeviceStatsMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stopping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_func\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmove_data_to_device\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_lightning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutilities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAllGatherGrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank_zero_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank_zero_only\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m from pytorch_lightning.utilities.enums import (  # noqa: F401\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/apply_func.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_TORCHTEXT_AVAILABLE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_compare_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0.9.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/imports.py\u001b[0m in \u001b[0;36m_compare_version\u001b[0;34m(package, op, version, use_base_version)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \"\"\"\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mpkg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlegacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m from torchtext._torchtext import (\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mVocab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mVocabPybind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.7/dist-packages/torchtext/_torchtext.so: undefined symbol: _ZN3c106ivalue6Future15extractDataPtrsERKNS_6IValueE",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}